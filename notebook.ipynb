{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rodrigo.almeida\\Documents\\GitHub\\goya\\env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rodrigo.almeida\\Documents\\GitHub\\goya\\env\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "c:\\Users\\rodrigo.almeida\\Documents\\GitHub\\goya\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.982073187828064}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"O diretor não aprovou a compra do carro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 126] Não foi possível encontrar o módulo especificado. Error loading \"c:\\Users\\rodrigo.almeida\\Documents\\GitHub\\goya\\.venv\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[1;32mc:\\Users\\rodrigo.almeida\\Documents\\GitHub\\goya\\.venv\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] Não foi possível encontrar o módulo especificado. Error loading \"c:\\Users\\rodrigo.almeida\\Documents\\GitHub\\goya\\.venv\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\rodrigo.almeida\\\\.cache\\\\huggingface\\\\hub\\\\models--stjiris--bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-MetaKD-v1\\\\snapshots\\\\703a80a525f2b56332420f26ee2f23b8d197158f'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(repo_id=\"stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-MetaKD-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rodrigo.almeida\\Documents\\GitHub\\goya\\env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodrigo.almeida\\Documents\\GitHub\\goya\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.571917   -0.5091136  -0.11547787 ...  1.0301194  -1.0442085\n",
      "  -0.7477183 ]\n",
      " [-1.1913153  -0.6908358   0.1747199  ...  1.0827097  -1.3476166\n",
      "  -0.32752785]]\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"Isto é um exemplo\", \"Isto é um outro exemplo\"]\n",
    "model = SentenceTransformer('C:\\\\Users\\\\rodrigo.almeida\\\\.cache\\\\huggingface\\\\hub\\\\models--stjiris--bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-MetaKD-v1\\\\snapshots\\\\703a80a525f2b56332420f26ee2f23b8d197158f')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"Isto é um exemplo\", \"Isto é um outro exemplo\"]\n",
    "model = SentenceTransformer('C:\\\\Users\\\\rodrigo.almeida\\\\.cache\\\\huggingface\\\\hub\\\\models--stjiris--bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-MetaKD-v1\\\\snapshots\\\\703a80a525f2b56332420f26ee2f23b8d197158f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: tensor([[-0.3661, -0.9497, -0.4922,  ..., -0.9804, -1.2134,  0.2775],\n",
      "        [-0.3014, -0.8197,  0.0579,  ..., -0.0351, -1.2885,  0.3856],\n",
      "        [ 0.1117, -0.8909,  0.0601,  ...,  0.4692, -1.5803,  0.5137],\n",
      "        ...,\n",
      "        [ 0.5380, -0.5338, -0.2628,  ..., -0.0130, -1.6632,  1.2649],\n",
      "        [ 0.7811, -1.0847,  0.4262,  ..., -0.1148, -1.5857,  1.0044],\n",
      "        [-0.3661, -0.9497, -0.4922,  ..., -0.9804, -1.2134,  0.2775]])\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: tensor([[ 1.0407, -0.4103, -1.1137,  ..., -1.2422, -0.6234, -0.5782],\n",
      "        [ 1.4293, -0.6560, -0.7859,  ..., -0.5574, -1.0486, -0.2033],\n",
      "        [ 1.3969, -1.2237, -0.7863,  ..., -0.6114, -0.9104, -0.5519],\n",
      "        ...,\n",
      "        [ 0.7123, -0.4283, -0.5843,  ..., -0.4412, -1.2044, -0.2764],\n",
      "        [ 0.6698, -0.6436, -0.5944,  ..., -0.4177, -1.1497, -0.2472],\n",
      "        [ 0.6039, -0.3390, -0.5676,  ..., -0.4679, -1.2667, -0.0682]])\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: tensor([[-0.5427, -0.3530,  0.1309,  ..., -0.5374,  0.3572, -0.0406],\n",
      "        [-0.2321, -0.0722, -0.0567,  ...,  0.5812, -0.6691,  0.3748],\n",
      "        [-0.1377, -0.4434,  0.5253,  ...,  0.7193, -0.1004,  0.2721],\n",
      "        ...,\n",
      "        [-0.5650,  0.2341,  0.0716,  ...,  1.3191,  0.0043,  0.6988],\n",
      "        [-0.5423, -0.3527,  0.1312,  ..., -0.5368,  0.3562, -0.0405],\n",
      "        [-0.5427, -0.3530,  0.1308,  ..., -0.5375,  0.3572, -0.0406]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Specify the local directory where the model is saved\n",
    "local_model_path = \"C:\\\\Users\\\\rodrigo.almeida\\\\.cache\\\\huggingface\\\\hub\\\\models--stjiris--bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-MetaKD-v1\\\\snapshots\\\\703a80a525f2b56332420f26ee2f23b8d197158f\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-MetaKD-v1\")\n",
    "# model = AutoModel.from_pretrained(\"stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-MetaKD-v1\")\n",
    "# Load the tokenizer and model from the local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModel.from_pretrained(local_model_path)\n",
    "\n",
    "# Our sentences to encode\n",
    "sentences = [\n",
    "    \"This framework generates embeddings for each input sentence\",\n",
    "    \"Sentences are passed as a list of string.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Pass the tokenized inputs to the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the embeddings (use the last hidden state)\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "\n",
    "# Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA Version: None\n",
      "Is CUDA available: False\n",
      "CUDA Device Name: No CUDA device found\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch CUDA Version:\", torch.version.cuda)\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA device found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodrigo.almeida\\Documents\\GitHub\\goya\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: [ 0.32541245 -0.7907751   0.03737818 ... -0.08492713 -1.4870802\n",
      "  0.7626551 ]\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: [ 1.2157315  -0.61772484 -0.886408   ... -0.72826034 -0.5921469\n",
      " -0.28579944]\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: [-0.35081843 -0.0968156   0.13904461 ...  0.68020695 -0.22156009\n",
      "  0.46256095]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('C:\\\\Users\\\\rodrigo.almeida\\\\.cache\\\\huggingface\\\\hub\\\\models--stjiris--bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-MetaKD-v1\\\\snapshots\\\\703a80a525f2b56332420f26ee2f23b8d197158f')\n",
    "\n",
    "# Our sentences to encode\n",
    "sentences = [\n",
    "    \"This framework generates embeddings for each input sentence\",\n",
    "    \"Sentences are passed as a list of string.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "]\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 most similar pairs:\n",
      "A man is riding a horse. \t A man is riding a white horse on an enclosed ground. \t 0.8119\n",
      "A man is eating a piece of bread. \t A man is riding a white horse on an enclosed ground. \t 0.6419\n",
      "A monkey is playing drums. \t Someone in a gorilla costume is playing a set of drums. \t 0.6213\n",
      "A man is eating a piece of bread. \t A man is riding a horse. \t 0.5563\n",
      "The girl is carrying a baby. \t Two men pushed carts through the woods. \t 0.4301\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('C:\\\\Users\\\\rodrigo.almeida\\\\.cache\\\\huggingface\\\\hub\\\\models--stjiris--bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-MetaKD-v1\\\\snapshots\\\\703a80a525f2b56332420f26ee2f23b8d197158f')\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"A man is eating food.\",\n",
    "    \"A man is eating a piece of bread.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "    \"A woman is playing violin.\",\n",
    "    \"Two men pushed carts through the woods.\",\n",
    "    \"A man is riding a white horse on an enclosed ground.\",\n",
    "    \"A monkey is playing drums.\",\n",
    "    \"Someone in a gorilla costume is playing a set of drums.\",\n",
    "]\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Compute cosine similarity between all pairs\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "# Add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim) - 1):\n",
    "    for j in range(i + 1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "# Sort list by the highest cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"Top-5 most similar pairs:\")\n",
    "for score, i, j in all_sentence_combinations[0:5]:\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(sentences[i], sentences[j], cos_sim[i][j]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
